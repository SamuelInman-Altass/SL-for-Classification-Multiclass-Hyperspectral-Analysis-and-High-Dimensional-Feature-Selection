# Statistical Learning for Classification: Multiclass Hyperspectral Analysis and High Dimensional Feature Selection
All of the code and the report were produced in equal parts by all (4) co-authors, Prithika Narayanan, Anastasiia Prokhorova, Samuel Inman-Altass, and Fatma Kassim.
#
DIRECT LINK TO THE RESEARCH REPORT HERE: https://drive.google.com/file/d/1WbjBVHKZ4120WeWCro6ZvMsJAuAhviXK/view?usp=sharing
#
**Abstract:** We apply statistical learning methods to two classification problems. First, we address multiclass classification on hyperspectral satellite imagery (215,604 pixels, 218 spectral bands, 420–2450 nm) from an alpine region in Tyrol, Austria. We evaluate seven classifiers with and without PCA (10 components) using macro-F1 due to class imbalance (4.7%–23% per class). Linear SVM achieves best performance (F1 = 0.995, BalAcc = 0.997), though Logistic Regression delivers nearly identical results (F1 = 0.994, BalAcc = 0.996) with 250× faster inference (0.14s vs 36s). For glacier-ice detection (binary task, 5% positive class), we optimize F2 score prioritizing recall, achieving near-perfect separation (F2 = 0.998, recall = 1.000) with Logistic Regression demonstrating that glacier ice has a highly distinctive spectral signature. Second, we study feature selection in a high-dimensional neural decoding task where the dataset exhibits extreme p ≫ n imbalance (p/n ≈ 16) and class imbalance (Left: 30.5%, Right: 69.5%), making stable generalisation difficult. We compare six feature selection paradigms: Forward Stepwise Selection (FSS), mRMR, Lasso, Elastic Net, Random Forest VI, and Gradient Boosting VI within a unified evaluation pipeline that ranks features, evaluates performance across subset sizes K, and assesses generalisation at optimal K∗. FSS yields the best overall trade-off, achieving strong performance (BalAcc = 0.774) with compact subsets (K∗ = 30). Gradient Boosting VI performs similarly (BalAcc = 0.729) with slightly larger subset (K∗ = 35), while Elastic Net achieves the highest BalAcc (0.805) at the cost of dense, less transferable solutions (K∗ = 2168). We discuss why these methods behave differently and outline limitations affecting reliability of our findings.
